{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Conv1D, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, LSTM,Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers import CuDNNLSTM, CuDNNGRU\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "from keras import optimizers\n",
    "from keras.layers import Lambda\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "import gc\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import time\n",
    "\n",
    "eng_stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = '../inputs/wiki.zh.vec'\n",
    "train = pd.read_csv(\"../inputs/train.tsv\",sep='\\t')\n",
    "test = pd.read_csv(\"../inputs/vali.tsv\",sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[\"内容\"].fillna(\"无\").str.lower()\n",
    "y_train = train[\"标签\"].values\n",
    "\n",
    "X_test = test[\"内容\"].fillna(\"无\").str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features=100000\n",
    "maxlen=800\n",
    "embed_size=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "lookupTable, y_train = np.unique(y_train, return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['人类作者', '机器作者', '机器翻译', '自动摘要'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookupTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features=100000\n",
    "maxlen=800\n",
    "embed_size=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok=text.Tokenizer(num_words=max_features)\n",
    "tok.fit_on_texts(list(X_train)+list(X_test))\n",
    "X_train=tok.texts_to_sequences(X_train)\n",
    "X_test=tok.texts_to_sequences(X_test)\n",
    "x_train=sequence.pad_sequences(X_train,maxlen=maxlen)\n",
    "x_test=sequence.pad_sequences(X_test,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(EMBEDDING_FILE,encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tok.word_index\n",
    "#prepare embedding matrix\n",
    "num_words = min(max_features, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(maxlen, ))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable = False)(sequence_input)\n",
    "x = SpatialDropout1D(0.2)(x)\n",
    "x = Bidirectional(GRU(128, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "x = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "avg_pool = GlobalAveragePooling1D()(x)\n",
    "max_pool = GlobalMaxPooling1D()(x)\n",
    "x = concatenate([avg_pool, max_pool]) \n",
    "preds = Dense(4, activation=\"softmax\")(x)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=1e-3),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "y_train = to_categorical(y_train, num_classes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaqiang/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 30\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.9, random_state=233)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"../inputs/weights_base.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n",
    "f1_val = RocAucEvaluation(validation_data=(X_val, y_val), interval = 1)\n",
    "callbacks_list = [f1_val,checkpoint, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124451 samples, validate on 13828 samples\n",
      "Epoch 1/30\n",
      "124451/124451 [==============================] - 1165s 9ms/step - loss: 0.0972 - acc: 0.9662 - val_loss: 0.0898 - val_acc: 0.9685\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.997784\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.96854, saving model to ../inputs/weights_base.best.hdf5\n",
      "Epoch 2/30\n",
      "124451/124451 [==============================] - 1175s 9ms/step - loss: 0.0877 - acc: 0.9701 - val_loss: 0.0947 - val_acc: 0.9690\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.997582\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.96854 to 0.96898, saving model to ../inputs/weights_base.best.hdf5\n",
      "Epoch 3/30\n",
      "124451/124451 [==============================] - 1315s 11ms/step - loss: 0.0794 - acc: 0.9726 - val_loss: 0.0693 - val_acc: 0.9770\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.998180\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.96898 to 0.97700, saving model to ../inputs/weights_base.best.hdf5\n",
      "Epoch 4/30\n",
      "124451/124451 [==============================] - 1511s 12ms/step - loss: 0.0729 - acc: 0.9754 - val_loss: 0.0722 - val_acc: 0.9759\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.998162\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.97700\n",
      "Epoch 5/30\n",
      "124451/124451 [==============================] - 1511s 12ms/step - loss: 0.0695 - acc: 0.9762 - val_loss: 0.0645 - val_acc: 0.9793\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.998489\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.97700 to 0.97932, saving model to ../inputs/weights_base.best.hdf5\n",
      "Epoch 6/30\n",
      "124451/124451 [==============================] - 1512s 12ms/step - loss: 0.0652 - acc: 0.9778 - val_loss: 0.0593 - val_acc: 0.9806\n",
      "\n",
      " ROC-AUC - epoch: 6 - score: 0.998528\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.97932 to 0.98062, saving model to ../inputs/weights_base.best.hdf5\n",
      "Epoch 7/30\n",
      "124451/124451 [==============================] - 1513s 12ms/step - loss: 0.0638 - acc: 0.9785 - val_loss: 0.0740 - val_acc: 0.9770\n",
      "\n",
      " ROC-AUC - epoch: 7 - score: 0.998137\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.98062\n",
      "Epoch 8/30\n",
      "124451/124451 [==============================] - 1522s 12ms/step - loss: 0.0599 - acc: 0.9800 - val_loss: 0.0674 - val_acc: 0.9795\n",
      "\n",
      " ROC-AUC - epoch: 8 - score: 0.998309\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.98062\n",
      "Epoch 9/30\n",
      "124451/124451 [==============================] - 1523s 12ms/step - loss: 0.0588 - acc: 0.9800 - val_loss: 0.0624 - val_acc: 0.9800\n",
      "\n",
      " ROC-AUC - epoch: 9 - score: 0.998457\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.98062\n",
      "Epoch 10/30\n",
      "124451/124451 [==============================] - 1526s 12ms/step - loss: 0.0577 - acc: 0.9806 - val_loss: 0.0554 - val_acc: 0.9818\n",
      "\n",
      " ROC-AUC - epoch: 10 - score: 0.998702\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.98062 to 0.98178, saving model to ../inputs/weights_base.best.hdf5\n",
      "Epoch 11/30\n",
      "124451/124451 [==============================] - 1524s 12ms/step - loss: 0.0558 - acc: 0.9813 - val_loss: 0.0588 - val_acc: 0.9829\n",
      "\n",
      " ROC-AUC - epoch: 11 - score: 0.998602\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.98178 to 0.98286, saving model to ../inputs/weights_base.best.hdf5\n",
      "Epoch 12/30\n",
      "124451/124451 [==============================] - 1526s 12ms/step - loss: 0.0528 - acc: 0.9822 - val_loss: 0.0713 - val_acc: 0.9763\n",
      "\n",
      " ROC-AUC - epoch: 12 - score: 0.998626\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.98286\n",
      "Epoch 13/30\n",
      "124451/124451 [==============================] - 1519s 12ms/step - loss: 0.0513 - acc: 0.9825 - val_loss: 0.0568 - val_acc: 0.9818\n",
      "\n",
      " ROC-AUC - epoch: 13 - score: 0.998780\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.98286\n",
      "Epoch 14/30\n",
      "124451/124451 [==============================] - 1528s 12ms/step - loss: 0.0496 - acc: 0.9833 - val_loss: 0.0600 - val_acc: 0.9809\n",
      "\n",
      " ROC-AUC - epoch: 14 - score: 0.998660\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.98286\n",
      "Epoch 15/30\n",
      "124451/124451 [==============================] - 1519s 12ms/step - loss: 0.0491 - acc: 0.9829 - val_loss: 0.0544 - val_acc: 0.9832\n",
      "\n",
      " ROC-AUC - epoch: 15 - score: 0.998757\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.98286 to 0.98315, saving model to ../inputs/weights_base.best.hdf5\n",
      "Epoch 16/30\n",
      "124451/124451 [==============================] - 1526s 12ms/step - loss: 0.0484 - acc: 0.9834 - val_loss: 0.0597 - val_acc: 0.9820\n",
      "\n",
      " ROC-AUC - epoch: 16 - score: 0.998629\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.98315\n",
      "Epoch 17/30\n",
      "124451/124451 [==============================] - 1521s 12ms/step - loss: 0.0467 - acc: 0.9842 - val_loss: 0.0597 - val_acc: 0.9832\n",
      "\n",
      " ROC-AUC - epoch: 17 - score: 0.998751\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.98315\n",
      "Epoch 18/30\n",
      "124451/124451 [==============================] - 1526s 12ms/step - loss: 0.0465 - acc: 0.9842 - val_loss: 0.0544 - val_acc: 0.9818\n",
      "\n",
      " ROC-AUC - epoch: 18 - score: 0.998940\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.98315\n",
      "Epoch 19/30\n",
      "124451/124451 [==============================] - 1528s 12ms/step - loss: 0.0447 - acc: 0.9847 - val_loss: 0.0569 - val_acc: 0.9824\n",
      "\n",
      " ROC-AUC - epoch: 19 - score: 0.998719\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.98315\n",
      "Epoch 20/30\n",
      "124451/124451 [==============================] - 1521s 12ms/step - loss: 0.0440 - acc: 0.9852 - val_loss: 0.0543 - val_acc: 0.9835\n",
      "\n",
      " ROC-AUC - epoch: 20 - score: 0.998840\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.98315 to 0.98351, saving model to ../inputs/weights_base.best.hdf5\n",
      "Epoch 21/30\n",
      "124451/124451 [==============================] - 1522s 12ms/step - loss: 0.0436 - acc: 0.9850 - val_loss: 0.0564 - val_acc: 0.9818\n",
      "\n",
      " ROC-AUC - epoch: 21 - score: 0.998706\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.98351\n",
      "Epoch 22/30\n",
      "124451/124451 [==============================] - 1523s 12ms/step - loss: 0.0437 - acc: 0.9848 - val_loss: 0.0575 - val_acc: 0.9824\n",
      "\n",
      " ROC-AUC - epoch: 22 - score: 0.998774\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.98351\n",
      "Epoch 23/30\n",
      "124451/124451 [==============================] - 1522s 12ms/step - loss: 0.0424 - acc: 0.9854 - val_loss: 0.0590 - val_acc: 0.9823\n",
      "\n",
      " ROC-AUC - epoch: 23 - score: 0.998813\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.98351\n",
      "Epoch 24/30\n",
      "124451/124451 [==============================] - 1524s 12ms/step - loss: 0.0429 - acc: 0.9855 - val_loss: 0.0578 - val_acc: 0.9832\n",
      "\n",
      " ROC-AUC - epoch: 24 - score: 0.998716\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.98351\n",
      "Epoch 25/30\n",
      "124451/124451 [==============================] - 1522s 12ms/step - loss: 0.0408 - acc: 0.9860 - val_loss: 0.0565 - val_acc: 0.9832\n",
      "\n",
      " ROC-AUC - epoch: 25 - score: 0.998883\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.98351\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2a0d079400>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),callbacks = callbacks_list,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting....\n",
      "58567/58567 [==============================] - 59s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(filepath)\n",
    "print('Predicting....')\n",
    "y_pred = model.predict(x_test,batch_size=1024,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = np.argmax(y_pred,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['人类作者', '机器作者', '机器翻译', '自动摘要'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookupTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58567,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = dict()\n",
    "for i, v in enumerate(lookupTable):\n",
    "    my_dict[i] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '人类作者', 1: '机器作者', 2: '机器翻译', 3: '自动摘要'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['标签'] = np.vectorize(my_dict.get)(y_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('../inputs/sub_bilistmcnn_1.csv', columns=['id', '标签'], header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
