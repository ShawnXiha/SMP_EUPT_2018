{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaqiang/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "np.random.seed(32)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "\n",
    "import logging\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = '../inputs/wiki.zh.vec'\n",
    "train = pd.read_csv(\"../inputs/train.tsv\",sep='\\t')\n",
    "test = pd.read_csv(\"../inputs/vali.tsv\",sep='\\t')\n",
    "X_train = train[\"内容\"].fillna(\"无\").str.lower()\n",
    "y_train = train[\"标签\"].values\n",
    "X_test = test[\"内容\"].fillna(\"无\").str.lower()\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features=100000\n",
    "maxlen=800\n",
    "embed_size=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "lookupTable, y_train = np.unique(y_train, return_inverse=True)\n",
    "y_train = to_categorical(y_train, num_classes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok=Tokenizer(num_words=max_features, filters=\"!\\\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n——！，。？、~@#￥%……&*（）：；《）《》“”()»〔〕-]+\")\n",
    "tok.fit_on_texts(list(X_train)+list(X_test))\n",
    "X_train=tok.texts_to_sequences(X_train)\n",
    "X_test=tok.texts_to_sequences(X_test)\n",
    "x_train=pad_sequences(X_train,maxlen=maxlen)\n",
    "x_test=pad_sequences(X_test,maxlen=maxlen)\n",
    "embeddings_index = {}\n",
    "with open(EMBEDDING_FILE,encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "word_index = tok.word_index\n",
    "#prepare embedding matrix\n",
    "num_words = min(max_features, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from keras.layers import GRU, BatchNormalization, Conv1D, MaxPooling1D\n",
    "\n",
    "file_path = \"best_model_bigru_cnn.hdf5\"\n",
    "check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                              save_best_only = True, mode = \"min\")\n",
    "early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):\n",
    "    inp = Input(shape = (maxlen,))\n",
    "    x = Embedding(max_features, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(dr)(x)\n",
    "\n",
    "    x = Bidirectional(GRU(units, return_sequences = True))(x1)\n",
    "    x = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
    "    \n",
    "    y = Bidirectional(LSTM(units, return_sequences = True))(x1)\n",
    "    y = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(y)\n",
    "    \n",
    "    avg_pool1 = GlobalAveragePooling1D()(x)\n",
    "    max_pool1 = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    avg_pool2 = GlobalAveragePooling1D()(y)\n",
    "    max_pool2 = GlobalMaxPooling1D()(y)\n",
    "    \n",
    "    \n",
    "    x = concatenate([avg_pool1, max_pool1, avg_pool2, max_pool2])\n",
    "\n",
    "    x = Dense(4, activation = \"softmax\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    history = model.fit(x_train, y_train, batch_size = 128, epochs = 30,validation_split=0.05 , \n",
    "                        verbose = 1, callbacks = [check_point, early_stop])\n",
    "    model = load_model(file_path)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 131365 samples, validate on 6914 samples\n",
      "Epoch 1/30\n",
      "131365/131365 [==============================] - 3314s 25ms/step - loss: 0.3263 - acc: 0.8672 - val_loss: 0.1889 - val_acc: 0.9293\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.18886, saving model to best_model_bigru_cnn.hdf5\n",
      "Epoch 2/30\n",
      "131365/131365 [==============================] - 3241s 25ms/step - loss: 0.1860 - acc: 0.9282 - val_loss: 0.1650 - val_acc: 0.9408\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.18886 to 0.16499, saving model to best_model_bigru_cnn.hdf5\n",
      "Epoch 3/30\n",
      "131365/131365 [==============================] - 3152s 24ms/step - loss: 0.1537 - acc: 0.9417 - val_loss: 0.1510 - val_acc: 0.9482\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.16499 to 0.15101, saving model to best_model_bigru_cnn.hdf5\n",
      "Epoch 4/30\n",
      "131365/131365 [==============================] - 3123s 24ms/step - loss: 0.1403 - acc: 0.9465 - val_loss: 0.1283 - val_acc: 0.9530\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.15101 to 0.12825, saving model to best_model_bigru_cnn.hdf5\n",
      "Epoch 5/30\n",
      "131365/131365 [==============================] - 3122s 24ms/step - loss: 0.1297 - acc: 0.9510 - val_loss: 0.1329 - val_acc: 0.9517\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.12825\n",
      "Epoch 6/30\n",
      "131365/131365 [==============================] - 3120s 24ms/step - loss: 0.1218 - acc: 0.9541 - val_loss: 0.1326 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.12825\n",
      "Epoch 7/30\n",
      "131365/131365 [==============================] - 3119s 24ms/step - loss: 0.1166 - acc: 0.9554 - val_loss: 0.1311 - val_acc: 0.9534\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.12825\n",
      "Epoch 8/30\n",
      "131365/131365 [==============================] - 3118s 24ms/step - loss: 0.1113 - acc: 0.9577 - val_loss: 0.1170 - val_acc: 0.9575\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.12825 to 0.11701, saving model to best_model_bigru_cnn.hdf5\n",
      "Epoch 9/30\n",
      "131365/131365 [==============================] - 3117s 24ms/step - loss: 0.1052 - acc: 0.9598 - val_loss: 0.1195 - val_acc: 0.9555\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.11701\n",
      "Epoch 10/30\n",
      "131365/131365 [==============================] - 3120s 24ms/step - loss: 0.1030 - acc: 0.9610 - val_loss: 0.1236 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.11701\n",
      "Epoch 11/30\n",
      " 64896/131365 [=============>................] - ETA: 26:15 - loss: 0.0975 - acc: 0.9624"
     ]
    }
   ],
   "source": [
    "model = build_model(lr = 1e-3, lr_d = 0, units = 128, dr = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
